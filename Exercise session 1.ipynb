{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, name=\"train\", path=\"dataset\"):\n",
    "        self.path = path\n",
    "\n",
    "        print(\"Reading dataset in %s/\" %path)\n",
    "        bodies = name+\"_bodies.csv\"\n",
    "        stances = name+\"_stances.csv\"\n",
    "\n",
    "        print(\"Loading files %s, %s\" %(bodies,stances))\n",
    "        self.data = self.read(bodies,stances)\n",
    "\n",
    "    def read(self,bodies,stances):\n",
    "        train_bodies = pd.read_csv(self.path + \"/\" + bodies)\n",
    "        train_stances = pd.read_csv(self.path + \"/\" + stances)\n",
    "        merged = train_bodies.merge(train_stances,left_on='Body ID',right_on='Body ID',how='outer')\n",
    "        return merged\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset in dataset/\n",
      "Loading files train_bodies.csv, train_stances.csv\n",
      "Reading dataset in dataset/\n",
      "Loading files competition_test_bodies.csv, competition_test_stances.csv\n"
     ]
    }
   ],
   "source": [
    "training = DataSet()\n",
    "test = DataSet(name=\"competition_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody  \\\n",
      "0        0  A small meteorite crashed into a wooded area i...   \n",
      "1        0  A small meteorite crashed into a wooded area i...   \n",
      "2        0  A small meteorite crashed into a wooded area i...   \n",
      "3        0  A small meteorite crashed into a wooded area i...   \n",
      "4        0  A small meteorite crashed into a wooded area i...   \n",
      "\n",
      "                                            Headline     Stance  \n",
      "0  Soldier shot, Parliament locked down after gun...  unrelated  \n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
      "2  Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
      "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
      "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  \n"
     ]
    }
   ],
   "source": [
    "print(training.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the text and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer() \n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def get_clean_tokens(s):\n",
    "    s = clean(s)\n",
    "    tokens = get_tokenized_lemmas(s)\n",
    "    clean_tokens = remove_stopwords(tokens)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.data['Headline tokens'] = training.data['Headline'].map(get_clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody  \\\n",
      "0        0  A small meteorite crashed into a wooded area i...   \n",
      "1        0  A small meteorite crashed into a wooded area i...   \n",
      "2        0  A small meteorite crashed into a wooded area i...   \n",
      "3        0  A small meteorite crashed into a wooded area i...   \n",
      "4        0  A small meteorite crashed into a wooded area i...   \n",
      "\n",
      "                                            Headline     Stance  \\\n",
      "0  Soldier shot, Parliament locked down after gun...  unrelated   \n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated   \n",
      "2  Luke Somers 'killed in failed rescue attempt i...  unrelated   \n",
      "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated   \n",
      "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated   \n",
      "\n",
      "                                     Headline tokens  \n",
      "0  [soldier, shot, parliament, locked, gunfire, e...  \n",
      "1  [tourist, dubbed, spider, man, spider, burrow,...  \n",
      "2  [luke, somers, killed, failed, rescue, attempt...  \n",
      "3   [breaking, soldier, shot, war, memorial, ottawa]  \n",
      "4  [giant, 8ft, 9in, catfish, weighing, 19, stone...  \n"
     ]
    }
   ],
   "source": [
    "print(training.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.data['articleBody tokens'] = training.data['articleBody'].map(get_clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training.data.to_pickle(\"trainingdatatokens.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Reload the dataframe because it takes a long time to transform the body :-) \n",
    "training.data = pd.read_pickle(\"trainingdatatokens.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody  \\\n",
      "0        0  A small meteorite crashed into a wooded area i...   \n",
      "1        0  A small meteorite crashed into a wooded area i...   \n",
      "2        0  A small meteorite crashed into a wooded area i...   \n",
      "3        0  A small meteorite crashed into a wooded area i...   \n",
      "4        0  A small meteorite crashed into a wooded area i...   \n",
      "\n",
      "                                            Headline     Stance  \\\n",
      "0  Soldier shot, Parliament locked down after gun...  unrelated   \n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated   \n",
      "2  Luke Somers 'killed in failed rescue attempt i...  unrelated   \n",
      "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated   \n",
      "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated   \n",
      "\n",
      "                                     Headline tokens  \\\n",
      "0  [soldier, shot, parliament, locked, gunfire, e...   \n",
      "1  [tourist, dubbed, spider, man, spider, burrow,...   \n",
      "2  [luke, somers, killed, failed, rescue, attempt...   \n",
      "3   [breaking, soldier, shot, war, memorial, ottawa]   \n",
      "4  [giant, 8ft, 9in, catfish, weighing, 19, stone...   \n",
      "\n",
      "                                  articleBody tokens  \n",
      "0  [small, meteorite, crashed, wooded, area, nica...  \n",
      "1  [small, meteorite, crashed, wooded, area, nica...  \n",
      "2  [small, meteorite, crashed, wooded, area, nica...  \n",
      "3  [small, meteorite, crashed, wooded, area, nica...  \n",
      "4  [small, meteorite, crashed, wooded, area, nica...  \n"
     ]
    }
   ],
   "source": [
    "print(training.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "min_len = 2 #tunable parameter\n",
    "\n",
    "count_words = Counter()\n",
    "for tokens in training.data['articleBody tokens'].values:\n",
    "    for token in tokens:\n",
    "        if len(token)>min_len:\n",
    "            count_words[token]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 134112),\n",
       " ('state', 44137),\n",
       " ('video', 43653),\n",
       " ('year', 42425),\n",
       " ('report', 40928),\n",
       " ('apple', 40879),\n",
       " ('time', 37308),\n",
       " ('isi', 36373),\n",
       " ('people', 35930),\n",
       " ('told', 35424)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize: from text to a vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = training.data['articleBody tokens'].values + training.data['Headline tokens'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listed_docs = list(training.data['articleBody'].values)\n",
    "listed_docs2 = list(training.data['Headline'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize - fit: Fit vocabulary means generate the vector space for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(2,2), min_df=5)\n",
    "vectorizer.fit(listed_docs)\n",
    "vectorizer.fit(listed_docs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize - transform: Represent a document on the vector space that was fit previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.data['tfidf headlines'] = vectorizer.transform(listed_docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training.data['tfidf bodies'] = vectorizer.transform(listed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody  \\\n",
      "0        0  A small meteorite crashed into a wooded area i...   \n",
      "1        0  A small meteorite crashed into a wooded area i...   \n",
      "2        0  A small meteorite crashed into a wooded area i...   \n",
      "3        0  A small meteorite crashed into a wooded area i...   \n",
      "4        0  A small meteorite crashed into a wooded area i...   \n",
      "\n",
      "                                            Headline     Stance  \\\n",
      "0  Soldier shot, Parliament locked down after gun...  unrelated   \n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated   \n",
      "2  Luke Somers 'killed in failed rescue attempt i...  unrelated   \n",
      "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated   \n",
      "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated   \n",
      "\n",
      "                                     Headline tokens  \\\n",
      "0  [soldier, shot, parliament, locked, gunfire, e...   \n",
      "1  [tourist, dubbed, spider, man, spider, burrow,...   \n",
      "2  [luke, somers, killed, failed, rescue, attempt...   \n",
      "3   [breaking, soldier, shot, war, memorial, ottawa]   \n",
      "4  [giant, 8ft, 9in, catfish, weighing, 19, stone...   \n",
      "\n",
      "                                  articleBody tokens  \\\n",
      "0  [small, meteorite, crashed, wooded, area, nica...   \n",
      "1  [small, meteorite, crashed, wooded, area, nica...   \n",
      "2  [small, meteorite, crashed, wooded, area, nica...   \n",
      "3  [small, meteorite, crashed, wooded, area, nica...   \n",
      "4  [small, meteorite, crashed, wooded, area, nica...   \n",
      "\n",
      "                                     tfidf headlines  \n",
      "0    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "1    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "2    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "3    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "4    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n"
     ]
    }
   ],
   "source": [
    "print(training.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data just to make it less heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#training.data.to_pickle('data_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.data = pd.read_pickle(\"data_tfidf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the vector of a document looks like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports. \n",
      "\n",
      "Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \n",
      "Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\n",
      "\n",
      "The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\n",
      "\n",
      "Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\n",
      "\n",
      "\"We have to study it more because it could be ice or rock,\" he said.\n",
      "\n",
      "Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\n",
      "\n",
      "Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\n",
      "\n",
      "\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\n",
      "\n",
      "The site of the crater is near Managua's international airport and an air force base. Only journalists from state media were allowed to visit it.\n"
     ]
    }
   ],
   "source": [
    "print(listed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6540)\t0.270362121695911\n",
      "  (0, 6328)\t0.40880455331186605\n",
      "  (0, 4678)\t0.20606023018732983\n",
      "  (0, 4582)\t0.23969388029179453\n",
      "  (0, 4532)\t0.24399338064741213\n",
      "  (0, 3150)\t0.25560080851563965\n",
      "  (0, 2570)\t0.2450334731861741\n",
      "  (0, 1830)\t0.2450334731861741\n",
      "  (0, 1653)\t0.47938776058358906\n",
      "  (0, 1127)\t0.21618092366193067\n",
      "  (0, 310)\t0.2798814335000886\n",
      "  (0, 68)\t0.2450334731861741\n"
     ]
    }
   ],
   "source": [
    "a = vectorizer.transform([listed_docs[0]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "a = vectorizer.transform([listed_docs[0]])\n",
    "b = vectorizer.transform([listed_docs[200]])\n",
    "print(cosine_similarity(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the headline closest to the first body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the headline closest to the first body:\n",
    "body = vectorizer.transform([listed_docs[0]])\n",
    "sim_new = 0\n",
    "sim_old = 0\n",
    "for indx, headline in enumerate(listed_docs2):\n",
    "    head_vectorized = vectorizer.transform([headline])\n",
    "    sim_new = cosine_similarity(body,head_vectorized)\n",
    "    if sim_new > sim_old:\n",
    "        sim_old = sim_new\n",
    "        most_similar_indx = indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.271787\n",
      "Most similar: Meteor Leaves 40-Foot Crater Near Managua's Airport\n",
      "Match for: A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports. \n",
      "\n",
      "Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \n",
      "Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\n",
      "\n",
      "The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\n",
      "\n",
      "Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\n",
      "\n",
      "\"We have to study it more because it could be ice or rock,\" he said.\n",
      "\n",
      "Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\n",
      "\n",
      "Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\n",
      "\n",
      "\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\n",
      "\n",
      "The site of the crater is near Managua's international airport and an air force base. Only journalists from state media were allowed to visit it.\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity: %f\" %sim_old)\n",
    "print(\"Most similar: %s\" %listed_docs2[most_similar_indx])\n",
    "print(\"Match for: %s\" %listed_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reintroduce the task\n",
    "\n",
    "Ok, so let's do some fake news detection.\n",
    "\n",
    "Given a headline and a body of text, we want to say whether these two are:\n",
    "\n",
    "* unrelated\n",
    "* agree with each other\n",
    "* disagree with each other\n",
    "* discuss each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features ?\n",
    "\n",
    "First, we have to generate features.\n",
    "\n",
    "Let's check:\n",
    "\n",
    "* Word overlap (between headline and body)\n",
    "* Refuting features: words which are refuting\n",
    "* Polarity features: words which contain polarity\n",
    "* n-gram\n",
    "* char-grams\n",
    "* co-occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def word_overlap_features(headlines, bodies):\n",
    "    # Computes the percentage of overlap between the headline and the body of text (numerical)\n",
    "    X = []\n",
    "    for headline, body in zip(headlines, bodies):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        features = [\n",
    "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
    "        X.append(features)\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.005376344086021506],\n",
       " [0.0],\n",
       " [0.00546448087431694],\n",
       " [0.01098901098901099],\n",
       " [0.04736842105263158],\n",
       " [0.010582010582010581],\n",
       " [0.016483516483516484],\n",
       " [0.027472527472527472],\n",
       " [0.016129032258064516],\n",
       " [0.011049723756906077]]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_overlap_features(list(training.data['Headline'].values[0:10]), list(training.data['articleBody'].values[0:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_polarity(tokens):\n",
    "    return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    \n",
    "def refuting_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    X = []\n",
    "    for headline, body in zip(headlines, bodies):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        features = []\n",
    "        features = [1 if word in clean_headline else 0 for word in _refuting_words] #how many refutting words in the headline\n",
    "        features += [1 if word in clean_body else 0 for word in _refuting_words]\n",
    "        #print(features)\n",
    "        X.append(features)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "refuting_features(list(training.data['Headline'].values)[0:1], list(training.data['articleBody'].values)[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polarity_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    \n",
    "    X = []\n",
    "    for headline, body in zip(headlines, bodies):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        features = []\n",
    "        features.append(calculate_polarity(clean_headline))\n",
    "        features.append(calculate_polarity(clean_body))\n",
    "        X.append(features)\n",
    "        #print(features)\n",
    "    return np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [0, 1],\n",
       "       [1, 1],\n",
       "       [0, 1],\n",
       "       [0, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_features(list(training.data['Headline'].values)[0:10], list(training.data['articleBody'].values)[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "def append_chargrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in text_body[:100]:\n",
    "            grams_first_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    features.append(grams_first_hits)\n",
    "    return features\n",
    "\n",
    "def append_ngrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    return features\n",
    "\n",
    "def hand_features(headlines, bodies):\n",
    "    def binary_co_occurence(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in clean(headline).split(\" \"):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "            if headline_token in clean(body)[:255]:\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def binary_co_occurence_stops(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def count_grams(headline, body):\n",
    "        # Count how many times an n-gram of the title\n",
    "        # appears in the entire body, and intro paragraph\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = clean(headline)\n",
    "        features = []\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
    "        return features\n",
    "\n",
    "    X = []\n",
    "    for headline, body in zip(headlines, bodies):\n",
    "        X.append(binary_co_occurence(headline, body)\n",
    "                 + binary_co_occurence_stops(headline, body)\n",
    "                 + count_grams(headline, body))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  18,\n",
       "  4,\n",
       "  2,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hand_features(list(training.data['Headline'].values)[0:1], list(training.data['articleBody'].values)[0:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        feats = feat_fn(headlines, bodies)\n",
    "        np.save(feature_file, feats)\n",
    "\n",
    "    return np.load(feature_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELSINT = {'agree':0, 'disagree':1, 'discuss':2, 'unrelated':3}\n",
    "\n",
    "def generate_features(dataset,name):\n",
    "    h, b, y = [],[],[]\n",
    "\n",
    "    y = [LABELSINT[label] for label in list(dataset['Stance'].values)]\n",
    "    h = list(dataset['Headline'].values)\n",
    "    b = list(dataset['articleBody'].values)\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\"+name+\".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\"+name+\".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\"+name+\".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\"+name+\".npy\")\n",
    "    \n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "F = generate_features(training.data,'train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Now, we choose our \"favourite\" classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.dependencies import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset in dataset/\n",
      "Loading files train_bodies.csv, train_stances.csv\n",
      "Reading dataset in dataset/\n",
      "Loading files competition_test_bodies.csv, competition_test_stances.csv\n",
      "Reading dataset in dataset/\n",
      "Loading files train_bodies.csv, train_stances.csv\n",
      "Reading dataset in dataset/\n",
      "Loading files competition_test_bodies.csv, competition_test_stances.csv\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       34833.0286            1.18m\n",
      "         2       31155.3700            1.12m\n",
      "         3       28188.2279            1.10m\n",
      "         4       25762.4758            1.08m\n",
      "         5       23754.6905            1.07m\n",
      "         6       22083.6100            1.07m\n",
      "         7       20689.5452            1.06m\n",
      "         8       19500.9426            1.05m\n",
      "         9       18515.5698            1.04m\n",
      "        10       17669.3099            1.03m\n",
      "        20       13606.0571           57.67s\n",
      "        30       12451.4618           53.91s\n",
      "        40       11978.3661           50.42s\n",
      "        50       11683.0116           46.97s\n",
      "        60       11471.6449           43.76s\n",
      "        70       11316.2527           40.51s\n",
      "        80       11194.8344           37.31s\n",
      "        90       11089.9090           34.05s\n",
      "       100       10989.5910           30.85s\n",
      "       200       10226.4778            0.00s\n",
      "Score for fold 0 was - 0.7951703848982594\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35410.0840            1.11m\n",
      "         2       31692.3197            1.09m\n",
      "         3       28685.6820            1.08m\n",
      "         4       26223.7543            1.07m\n",
      "         5       24194.0940            1.06m\n",
      "         6       22503.1504            1.05m\n",
      "         7       21076.3082            1.04m\n",
      "         8       19885.4291            1.04m\n",
      "         9       18882.0819            1.03m\n",
      "        10       18020.9454            1.02m\n",
      "        20       13926.3919           57.86s\n",
      "        30       12738.1203           54.47s\n",
      "        40       12214.9623           50.93s\n",
      "        50       11929.4361           47.38s\n",
      "        60       11715.9794           44.02s\n",
      "        70       11541.6371           40.72s\n",
      "        80       11400.4510           37.47s\n",
      "        90       11277.4216           34.22s\n",
      "       100       11158.5037           31.02s\n",
      "       200       10379.7864            0.00s\n",
      "Score for fold 1 was - 0.7988545886296969\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35687.2318            1.30m\n",
      "         2       31937.8628            1.25m\n",
      "         3       28911.4199            1.22m\n",
      "         4       26432.4310            1.21m\n",
      "         5       24385.9358            1.19m\n",
      "         6       22678.2701            1.18m\n",
      "         7       21243.4162            1.17m\n",
      "         8       20041.4265            1.16m\n",
      "         9       19016.4699            1.15m\n",
      "        10       18148.7076            1.14m\n",
      "        20       14024.5343            1.04m\n",
      "        30       12763.9342           58.34s\n",
      "        40       12253.4099           54.18s\n",
      "        50       11951.0590           50.27s\n",
      "        60       11728.0812           46.56s\n",
      "        70       11555.6737           42.93s\n",
      "        80       11424.5454           39.40s\n",
      "        90       11298.7434           35.98s\n",
      "       100       11197.0246           32.61s\n",
      "       200       10440.3417            0.00s\n",
      "Score for fold 2 was - 0.8093922651933702\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35248.8563            1.23m\n",
      "         2       31556.1421            1.19m\n",
      "         3       28568.2038            1.17m\n",
      "         4       26127.3597            1.16m\n",
      "         5       24110.5778            1.15m\n",
      "         6       22421.3140            1.14m\n",
      "         7       21019.7023            1.13m\n",
      "         8       19828.8576            1.12m\n",
      "         9       18818.7421            1.12m\n",
      "        10       17982.9961            1.11m\n",
      "        20       13884.5674            1.02m\n",
      "        30       12693.0513           56.92s\n",
      "        40       12203.4447           52.89s\n",
      "        50       11928.0678           49.03s\n",
      "        60       11708.5482           46.03s\n",
      "        70       11551.4189           42.63s\n",
      "        80       11393.2153           40.14s\n",
      "        90       11276.3178           37.56s\n",
      "       100       11166.9226           34.27s\n",
      "       200       10371.0148            0.00s\n",
      "Score for fold 3 was - 0.8098772095533666\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35501.9627            1.55m\n",
      "         2       31764.0997            1.47m\n",
      "         3       28752.0981            1.40m\n",
      "         4       26287.9950            1.37m\n",
      "         5       24246.2393            1.34m\n",
      "         6       22549.9720            1.31m\n",
      "         7       21133.0474            1.28m\n",
      "         8       19933.9196            1.26m\n",
      "         9       18918.5695            1.30m\n",
      "        10       18074.2395            1.29m\n",
      "        20       13969.2721            1.30m\n",
      "        30       12786.2049            1.30m\n",
      "        40       12293.3864            1.18m\n",
      "        50       11983.3008            1.08m\n",
      "        60       11755.4312            1.01m\n",
      "        70       11572.4601           56.88s\n",
      "        80       11439.4227           52.63s\n",
      "        90       11320.9439           47.99s\n",
      "       100       11214.0948           43.14s\n",
      "       200       10419.8906            0.00s\n",
      "Score for fold 4 was - 0.7925311203319502\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35770.0145            1.54m\n",
      "         2       31975.5142            1.36m\n",
      "         3       28907.8347            1.50m\n",
      "         4       26396.7034            1.44m\n",
      "         5       24326.6141            1.52m\n",
      "         6       22585.8741            1.52m\n",
      "         7       21141.8313            1.58m\n",
      "         8       19922.9849            1.53m\n",
      "         9       18895.2941            1.50m\n",
      "        10       18011.7457            1.46m\n",
      "        20       13831.9376            1.35m\n",
      "        30       12630.1091            1.27m\n",
      "        40       12101.9858            1.21m\n",
      "        50       11805.7890            1.14m\n",
      "        60       11603.3618            1.03m\n",
      "        70       11428.0884           55.90s\n",
      "        80       11287.2869           50.90s\n",
      "        90       11162.5219           46.69s\n",
      "       100       11055.9587           42.22s\n",
      "       200       10280.7727            0.00s\n",
      "Score for fold 5 was - 0.7661266568483064\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35378.5710            1.67m\n",
      "         2       31641.1030            1.88m\n",
      "         3       28626.7006            1.71m\n",
      "         4       26160.8644            1.67m\n",
      "         5       24120.3373            1.59m\n",
      "         6       22428.3025            1.71m\n",
      "         7       21001.7377            1.79m\n",
      "         8       19814.3566            1.77m\n",
      "         9       18809.3098            1.77m\n",
      "        10       17938.9353            1.79m\n",
      "        20       13844.1893            1.61m\n",
      "        30       12662.6359            1.37m\n",
      "        40       12145.1733            1.22m\n",
      "        50       11846.7362            1.10m\n",
      "        60       11630.1594           59.39s\n",
      "        70       11463.1714           53.45s\n",
      "        80       11313.8004           48.31s\n",
      "        90       11189.3604           43.40s\n",
      "       100       11072.6484           39.39s\n",
      "       200       10326.3639            0.00s\n",
      "Score for fold 6 was - 0.767451881643206\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35550.3179            1.39m\n",
      "         2       31801.0364            1.35m\n",
      "         3       28767.5355            1.39m\n",
      "         4       26287.5379            1.34m\n",
      "         5       24230.4678            1.30m\n",
      "         6       22521.0619            1.28m\n",
      "         7       21086.9942            1.26m\n",
      "         8       19880.2324            1.26m\n",
      "         9       18851.9379            1.25m\n",
      "        10       17985.4966            1.26m\n",
      "        20       13857.9461            1.18m\n",
      "        30       12643.3712            1.07m\n",
      "        40       12097.9332            1.03m\n",
      "        50       11790.0175           59.65s\n",
      "        60       11558.0817           55.09s\n",
      "        70       11376.7414           50.65s\n",
      "        80       11236.2519           46.70s\n",
      "        90       11109.1508           42.71s\n",
      "       100       10997.7708           38.72s\n",
      "       200       10242.3339            0.00s\n",
      "Score for fold 7 was - 0.8000282845424975\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       34945.1789            1.37m\n",
      "         2       31279.4433            1.34m\n",
      "         3       28323.7009            1.43m\n",
      "         4       25907.4963            1.35m\n",
      "         5       23910.2252            1.40m\n",
      "         6       22247.3757            1.47m\n",
      "         7       20857.1601            1.44m\n",
      "         8       19685.8714            1.40m\n",
      "         9       18688.0998            1.43m\n",
      "        10       17835.5662            1.44m\n",
      "        20       13838.6522            1.23m\n",
      "        30       12622.8964            1.12m\n",
      "        40       12114.2659            1.02m\n",
      "        50       11832.3280           56.14s\n",
      "        60       11610.6719           52.05s\n",
      "        70       11441.7762           47.51s\n",
      "        80       11295.5397           43.80s\n",
      "        90       11177.0391           39.90s\n",
      "       100       11058.7182           35.97s\n",
      "       200       10320.6563            0.00s\n",
      "Score for fold 8 was - 0.8204444444444444\n",
      "      Iter       Train Loss   Remaining Time \n",
      "         1       35466.1102            1.09m\n",
      "         2       31720.4345            1.11m\n",
      "         3       28699.8711            1.10m\n",
      "         4       26224.6524            1.09m\n",
      "         5       24182.9995            1.08m\n",
      "         6       22492.1512            1.08m\n",
      "         7       21064.1562            1.07m\n",
      "         8       19877.9433            1.07m\n",
      "         9       18857.0460            1.07m\n",
      "        10       17988.1509            1.06m\n",
      "        20       13886.3771            1.14m\n",
      "        30       12691.1363            1.12m\n",
      "        40       12192.6420            1.10m\n",
      "        50       11910.9829            1.05m\n",
      "        60       11676.1073           59.29s\n",
      "        70       11504.3338           53.92s\n",
      "        80       11355.5284           49.03s\n",
      "        90       11231.1916           44.47s\n",
      "       100       11133.4330           40.35s\n",
      "       200       10384.6362            0.00s\n",
      "Score for fold 9 was - 0.7797934691599219\n",
      "Scores on the dev set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    124    |     3     |    551    |    84     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    17     |    10     |    121    |    14     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    62     |    13     |   1520    |    205    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     4     |     2     |    100    |   6792    |\n",
      "-------------------------------------------------------------\n",
      "Score: 3543.75 out of 4448.5\t(79.66168371361132%)\n",
      "\n",
      "\n",
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    205    |    15     |   1395    |    288    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    52     |    31     |    363    |    251    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    252    |    35     |   3494    |    683    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    13     |     1     |    352    |   17983   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8753.75 out of 11651.25\t(75.13142366698852%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.13142366698852"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "LABELS_RELATED = ['unrelated','related']\n",
    "RELATED = LABELS[0:3]\n",
    "#check_version()\n",
    "#parse_params()\n",
    "#Load the training dataset and generate folds\n",
    "d = DataSet(\"train\")\n",
    "folds,hold_out = kfold_split(d,n_folds=10)\n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)\n",
    "\n",
    "# Load the competition dataset\n",
    "competition_dataset = DataSet(\"competition_test\")\n",
    "X_competition, y_competition = generate_features(competition_dataset.data, \"competition\")\n",
    "\n",
    "training = DataSet()\n",
    "test = DataSet(name=\"competition_test\")\n",
    "Xs = dict()\n",
    "ys = dict()\n",
    "\n",
    "# Load/Precompute all features now\n",
    "X_holdout,y_holdout = generate_features(hold_out_stances,\"holdout\")\n",
    "for fold in fold_stances:\n",
    "    Xs[fold],ys[fold] = generate_features(fold_stances[fold],str(fold))\n",
    "\n",
    "best_score = 0\n",
    "best_fold = None\n",
    "\n",
    "# Classifier for each fold\n",
    "for fold in fold_stances:\n",
    "    ids = list(range(len(folds)))\n",
    "    del ids[fold]\n",
    "\n",
    "    X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
    "    y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
    "\n",
    "    X_test = Xs[fold]\n",
    "    y_test = ys[fold]\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = [LABELS[int(a)] for a in clf.predict(X_test)]\n",
    "    actual = [LABELS[int(a)] for a in y_test]\n",
    "\n",
    "    fold_score, _ = score_submission(actual, predicted)\n",
    "    max_fold_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    score = fold_score/float(max_fold_score)\n",
    "\n",
    "    print(\"Score for fold \"+ str(fold) + \" was - \" + str(score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_fold = clf\n",
    "\n",
    "\n",
    "#Run on Holdout set and report the final score on the holdout set\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_holdout)]\n",
    "actual = [LABELS[int(a)] for a in y_holdout]\n",
    "\n",
    "print(\"Scores on the dev set\")\n",
    "report_score(actual,predicted)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "#Run on competition dataset\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_competition)]\n",
    "actual = [LABELS[int(a)] for a in y_competition]\n",
    "\n",
    "print(\"Scores on the test set\")\n",
    "report_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    205    |    15     |   1395    |    288    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    52     |    31     |    363    |    251    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    252    |    35     |   3494    |    683    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    13     |     1     |    352    |   17983   |\n",
      "-------------------------------------------------------------\n",
      "Score: 8753.75 out of 11651.25\t(75.13142366698852%)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75.13142366698852"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run on competition dataset\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_competition)]\n",
    "actual = [LABELS[int(a)] for a in y_competition]\n",
    "\n",
    "print(\"Scores on the test set\")\n",
    "report_score(actual,predicted)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
