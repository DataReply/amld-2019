{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, name=\"train\", path=\"dataset\"):\n",
    "        self.path = path\n",
    "\n",
    "        print(\"Reading dataset in %s/\" %path)\n",
    "        bodies = name+\"_bodies.csv\"\n",
    "        stances = name+\"_stances.csv\"\n",
    "\n",
    "        print(\"Loading files %s, %s\" %(bodies,stances))\n",
    "        self.data = self.read(bodies,stances)\n",
    "\n",
    "    def read(self,bodies,stances):\n",
    "        train_bodies = pd.read_csv(self.path + \"/\" + bodies)\n",
    "        train_stances = pd.read_csv(self.path + \"/\" + stances)\n",
    "        merged = train_bodies.merge(train_stances,left_on='Body ID',right_on='Body ID',how='outer')\n",
    "        return merged\n",
    "    \n",
    "    def print(self):\n",
    "        print(self.data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset in dataset/\n",
      "Loading files train_bodies.csv, train_stances.csv\n",
      "Reading dataset in dataset/\n",
      "Loading files competition_test_bodies.csv, competition_test_stances.csv\n"
     ]
    }
   ],
   "source": [
    "training = DataSet()\n",
    "test = DataSet(name=\"competition_test\")\n",
    "#training.data.to_pickle(\"training_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn import feature_extraction\n",
    "from tqdm import tqdm\n",
    "\n",
    "_wnl = nltk.WordNetLemmatizer() \n",
    "\n",
    "def normalize_word(w):\n",
    "    return _wnl.lemmatize(w).lower()\n",
    "\n",
    "def get_tokenized_lemmas(s):\n",
    "    return [normalize_word(t) for t in nltk.word_tokenize(s)]\n",
    "\n",
    "def clean(s):\n",
    "    # Cleans a string: Lowercasing, trimming, removing non-alphanumeric\n",
    "    return \" \".join(re.findall(r'\\w+', s, flags=re.UNICODE)).lower()\n",
    "\n",
    "def remove_stopwords(l):\n",
    "    # Removes stopwords from a list of tokens\n",
    "    return [w for w in l if w not in feature_extraction.text.ENGLISH_STOP_WORDS]\n",
    "\n",
    "def get_clean_tokens(s):\n",
    "    s = clean(s)\n",
    "    tokens = get_tokenized_lemmas(s)\n",
    "    clean_tokens = remove_stopwords(tokens)\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training.data['articleBody tokens'] = training.data['articleBody'].map(get_clean_tokens)\n",
    "training.data['Headline tokens'] = training.data['Headline'].map(get_clean_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#tunable parameter\n",
    "min_len = 2\n",
    "\n",
    "count_words = Counter()\n",
    "for tokens in training.data['articleBody tokens'].values:\n",
    "    for token in tokens:\n",
    "        if len(token)>min_len:\n",
    "            count_words[token]+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 134112),\n",
       " ('state', 44137),\n",
       " ('video', 43653),\n",
       " ('year', 42425),\n",
       " ('report', 40928),\n",
       " ('apple', 40879),\n",
       " ('time', 37308),\n",
       " ('isi', 36373),\n",
       " ('people', 35930),\n",
       " ('told', 35424)]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_words.most_common(10)\n",
    "# word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = training.data['articleBody tokens'].values + training.data['Headline tokens'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "listed_docs = list(training.data['articleBody'].values)\n",
    "listed_docs2 = list(training.data['Headline'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize - fit: Fit vocabulary means generate the vector space for the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(2, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=\"english\", ngram_range=(2,2), min_df=5)\n",
    "vectorizer.fit(listed_docs)\n",
    "vectorizer.fit(listed_docs2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize - transform: Represent a document on the vector space that was fit previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.data['tfidf bodies'] = vectorizer.transform(listed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.data['tfidf headlines'] = vectorizer.transform(listed_docs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Body ID                                        articleBody  \\\n",
      "0        0  A small meteorite crashed into a wooded area i...   \n",
      "1        0  A small meteorite crashed into a wooded area i...   \n",
      "2        0  A small meteorite crashed into a wooded area i...   \n",
      "3        0  A small meteorite crashed into a wooded area i...   \n",
      "4        0  A small meteorite crashed into a wooded area i...   \n",
      "\n",
      "                                            Headline     Stance  \\\n",
      "0  Soldier shot, Parliament locked down after gun...  unrelated   \n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated   \n",
      "2  Luke Somers 'killed in failed rescue attempt i...  unrelated   \n",
      "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated   \n",
      "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated   \n",
      "\n",
      "                                  articleBody tokens  \\\n",
      "0  [small, meteorite, crashed, wooded, area, nica...   \n",
      "1  [small, meteorite, crashed, wooded, area, nica...   \n",
      "2  [small, meteorite, crashed, wooded, area, nica...   \n",
      "3  [small, meteorite, crashed, wooded, area, nica...   \n",
      "4  [small, meteorite, crashed, wooded, area, nica...   \n",
      "\n",
      "                                     Headline tokens  \\\n",
      "0  [soldier, shot, parliament, locked, gunfire, e...   \n",
      "1  [tourist, dubbed, spider, man, spider, burrow,...   \n",
      "2  [luke, somers, killed, failed, rescue, attempt...   \n",
      "3   [breaking, soldier, shot, war, memorial, ottawa]   \n",
      "4  [giant, 8ft, 9in, catfish, weighing, 19, stone...   \n",
      "\n",
      "                                        tfidf bodies  \\\n",
      "0    (0, 6540)\\t0.270362121695911\\n  (0, 6328)\\t0...   \n",
      "1    (0, 6540)\\t0.270362121695911\\n  (0, 6328)\\t0...   \n",
      "2    (0, 6540)\\t0.270362121695911\\n  (0, 6328)\\t0...   \n",
      "3    (0, 6540)\\t0.270362121695911\\n  (0, 6328)\\t0...   \n",
      "4    (0, 6540)\\t0.270362121695911\\n  (0, 6328)\\t0...   \n",
      "\n",
      "                                     tfidf headlines  \n",
      "0    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "1    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "2    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "3    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n",
      "4    (0, 7416)\\t0.2804843972745596\\n  (0, 6382)\\t...  \n"
     ]
    }
   ],
   "source": [
    "print(training.data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data just to make it less heavy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training.data.to_pickle('data_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports. \n",
      "\n",
      "Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \n",
      "Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\n",
      "\n",
      "The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\n",
      "\n",
      "Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\n",
      "\n",
      "\"We have to study it more because it could be ice or rock,\" he said.\n",
      "\n",
      "Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\n",
      "\n",
      "Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\n",
      "\n",
      "\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\n",
      "\n",
      "The site of the crater is near Managua's international airport and an air force base. Only journalists from state media were allowed to visit it.\n"
     ]
    }
   ],
   "source": [
    "print(listed_docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6540)\t0.270362121695911\n",
      "  (0, 6328)\t0.40880455331186605\n",
      "  (0, 4678)\t0.20606023018732983\n",
      "  (0, 4582)\t0.23969388029179453\n",
      "  (0, 4532)\t0.24399338064741213\n",
      "  (0, 3150)\t0.25560080851563965\n",
      "  (0, 2570)\t0.2450334731861741\n",
      "  (0, 1830)\t0.2450334731861741\n",
      "  (0, 1653)\t0.47938776058358906\n",
      "  (0, 1127)\t0.21618092366193067\n",
      "  (0, 310)\t0.2798814335000886\n",
      "  (0, 68)\t0.2450334731861741\n"
     ]
    }
   ],
   "source": [
    "a = vectorizer.transform([listed_docs[0]])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring similarity between two documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "a = vectorizer.transform([listed_docs[0]])\n",
    "b = vectorizer.transform([listed_docs[200]])\n",
    "print(cosine_similarity(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the headline closest to the first body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# find the headline closest to the first body:\n",
    "body = vectorizer.transform([listed_docs[0]])\n",
    "sim_new = 0\n",
    "sim_old = 0\n",
    "for indx, headline in enumerate(listed_docs2):\n",
    "    head_vectorized = vectorizer.transform([headline])\n",
    "    sim_new = cosine_similarity(body,head_vectorized)\n",
    "    if sim_new > sim_old:\n",
    "        sim_old = sim_new\n",
    "        most_similar_indx = indx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.271787\n",
      "Most similar: Meteor Leaves 40-Foot Crater Near Managua's Airport\n",
      "Match for: A small meteorite crashed into a wooded area in Nicaragua's capital of Managua overnight, the government said Sunday. Residents reported hearing a mysterious boom that left a 16-foot deep crater near the city's airport, the Associated Press reports. \n",
      "\n",
      "Government spokeswoman Rosario Murillo said a committee formed by the government to study the event determined it was a \"relatively small\" meteorite that \"appears to have come off an asteroid that was passing close to Earth.\" House-sized asteroid 2014 RC, which measured 60 feet in diameter, skimmed the Earth this weekend, ABC News reports. \n",
      "Murillo said Nicaragua will ask international experts to help local scientists in understanding what happened.\n",
      "\n",
      "The crater left by the meteorite had a radius of 39 feet and a depth of 16 feet,  said Humberto Saballos, a volcanologist with the Nicaraguan Institute of Territorial Studies who was on the committee. He said it is still not clear if the meteorite disintegrated or was buried.\n",
      "\n",
      "Humberto Garcia, of the Astronomy Center at the National Autonomous University of Nicaragua, said the meteorite could be related to an asteroid that was forecast to pass by the planet Saturday night.\n",
      "\n",
      "\"We have to study it more because it could be ice or rock,\" he said.\n",
      "\n",
      "Wilfried Strauch, an adviser to the Institute of Territorial Studies, said it was \"very strange that no one reported a streak of light. We have to ask if anyone has a photo or something.\"\n",
      "\n",
      "Local residents reported hearing a loud boom Saturday night, but said they didn't see anything strange in the sky.\n",
      "\n",
      "\"I was sitting on my porch and I saw nothing, then all of a sudden I heard a large blast. We thought it was a bomb because we felt an expansive wave,\" Jorge Santamaria told The Associated Press.\n",
      "\n",
      "The site of the crater is near Managua's international airport and an air force base. Only journalists from state media were allowed to visit it.\n"
     ]
    }
   ],
   "source": [
    "print(\"Similarity: %f\" %sim_old)\n",
    "print(\"Most similar: %s\" %listed_docs2[most_similar_indx])\n",
    "print(\"Match for: %s\" %listed_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reintroduce the task\n",
    "\n",
    "Ok, so let's do some fake news detection.\n",
    "\n",
    "Given a headline and a body of text, we want to say whether these two are:\n",
    "\n",
    "* unrelated\n",
    "* agree with each other\n",
    "* disagree with each other\n",
    "* discuss each other\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features ?\n",
    "\n",
    "First, we have to generate features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def word_overlap_features(headlines, bodies):\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        features = [\n",
    "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "def gen_or_load_feats(feat_fn, headlines, bodies, feature_file):\n",
    "    if not os.path.isfile(feature_file):\n",
    "        feats = feat_fn(headlines, bodies)\n",
    "        np.save(feature_file, feats)\n",
    "\n",
    "    return np.load(feature_file)\n",
    "\n",
    "\n",
    "def word_overlap_features(headlines, bodies):\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        clean_body = get_tokenized_lemmas(clean_body)\n",
    "        features = [\n",
    "            len(set(clean_headline).intersection(clean_body)) / float(len(set(clean_headline).union(clean_body)))]\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "def refuting_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        # 'refute',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_headline = get_tokenized_lemmas(clean_headline)\n",
    "        features = [1 if word in clean_headline else 0 for word in _refuting_words] #how many refutting words in the headline\n",
    "        X.append(features)\n",
    "    return X\n",
    "\n",
    "\n",
    "def polarity_features(headlines, bodies):\n",
    "    _refuting_words = [\n",
    "        'fake',\n",
    "        'fraud',\n",
    "        'hoax',\n",
    "        'false',\n",
    "        'deny', 'denies',\n",
    "        'not',\n",
    "        'despite',\n",
    "        'nope',\n",
    "        'doubt', 'doubts',\n",
    "        'bogus',\n",
    "        'debunk',\n",
    "        'pranks',\n",
    "        'retract'\n",
    "    ]\n",
    "\n",
    "    def calculate_polarity(text):\n",
    "        tokens = get_tokenized_lemmas(text)\n",
    "        return sum([t in _refuting_words for t in tokens]) % 2\n",
    "    \n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        clean_headline = clean(headline)\n",
    "        clean_body = clean(body)\n",
    "        features = []\n",
    "        features.append(calculate_polarity(clean_headline))\n",
    "        features.append(calculate_polarity(clean_body))\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "\n",
    "def ngrams(input, n):\n",
    "    input = input.split(' ')\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def chargrams(input, n):\n",
    "    output = []\n",
    "    for i in range(len(input) - n + 1):\n",
    "        output.append(input[i:i + n])\n",
    "    return output\n",
    "\n",
    "\n",
    "def append_chargrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in chargrams(\" \".join(remove_stopwords(text_headline.split())), size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    grams_first_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "        if gram in text_body[:100]:\n",
    "            grams_first_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    features.append(grams_first_hits)\n",
    "    return features\n",
    "\n",
    "def append_ngrams(features, text_headline, text_body, size):\n",
    "    grams = [' '.join(x) for x in ngrams(text_headline, size)]\n",
    "    grams_hits = 0\n",
    "    grams_early_hits = 0\n",
    "    for gram in grams:\n",
    "        if gram in text_body:\n",
    "            grams_hits += 1\n",
    "        if gram in text_body[:255]:\n",
    "            grams_early_hits += 1\n",
    "    features.append(grams_hits)\n",
    "    features.append(grams_early_hits)\n",
    "    return features\n",
    "\n",
    "def hand_features(headlines, bodies):\n",
    "    def binary_co_occurence(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in clean(headline).split(\" \"):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "            if headline_token in clean(body)[:255]:\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def binary_co_occurence_stops(headline, body):\n",
    "        # Count how many times a token in the title\n",
    "        # appears in the body text. Stopwords in the title\n",
    "        # are ignored.\n",
    "        bin_count = 0\n",
    "        bin_count_early = 0\n",
    "        for headline_token in remove_stopwords(clean(headline).split(\" \")):\n",
    "            if headline_token in clean(body):\n",
    "                bin_count += 1\n",
    "                bin_count_early += 1\n",
    "        return [bin_count, bin_count_early]\n",
    "\n",
    "    def count_grams(headline, body):\n",
    "        # Count how many times an n-gram of the title\n",
    "        # appears in the entire body, and intro paragraph\n",
    "        clean_body = clean(body)\n",
    "        clean_headline = clean(headline)\n",
    "        features = []\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 8)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_chargrams(features, clean_headline, clean_body, 16)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 2)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 3)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 4)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 5)\n",
    "        features = append_ngrams(features, clean_headline, clean_body, 6)\n",
    "        return features\n",
    "\n",
    "    X = []\n",
    "    for i, (headline, body) in tqdm(enumerate(zip(headlines, bodies))):\n",
    "        X.append(binary_co_occurence(headline, body)\n",
    "                 + binary_co_occurence_stops(headline, body)\n",
    "                 + count_grams(headline, body))\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def generate_features(stances,dataset,name):\n",
    "    h, b, y = [],[],[]\n",
    "\n",
    "    for stance in stances:\n",
    "        y.append(LABELS.index(stance['Stance']))\n",
    "        h.append(stance['Headline'])\n",
    "        b.append(dataset.articles[stance['Body ID']])\n",
    "\n",
    "    X_overlap = gen_or_load_feats(word_overlap_features, h, b, \"features/overlap.\"+name+\".npy\")\n",
    "    X_refuting = gen_or_load_feats(refuting_features, h, b, \"features/refuting.\"+name+\".npy\")\n",
    "    X_polarity = gen_or_load_feats(polarity_features, h, b, \"features/polarity.\"+name+\".npy\")\n",
    "    X_hand = gen_or_load_feats(hand_features, h, b, \"features/hand.\"+name+\".npy\")\n",
    "\n",
    "    X = np.c_[X_hand, X_polarity, X_refuting, X_overlap]\n",
    "    return X,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "Now, we choose our \"favourite\" classification algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading dataset in dataset/\n",
      "Loading files train_bodies.csv, train_stances.csv\n",
      "   Body ID                                        articleBody  \\\n",
      "0        0  A small meteorite crashed into a wooded area i...   \n",
      "1        0  A small meteorite crashed into a wooded area i...   \n",
      "2        0  A small meteorite crashed into a wooded area i...   \n",
      "3        0  A small meteorite crashed into a wooded area i...   \n",
      "4        0  A small meteorite crashed into a wooded area i...   \n",
      "\n",
      "                                            Headline     Stance  \n",
      "0  Soldier shot, Parliament locked down after gun...  unrelated  \n",
      "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
      "2  Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
      "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
      "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'kfold_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-158-661dd7b1821d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#Load the training dataset and generate folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhold_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfold_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mfold_stances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhold_out_stances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_stances_for_folds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhold_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kfold_split' is not defined"
     ]
    }
   ],
   "source": [
    "#check_version()\n",
    "#parse_params()\n",
    "\n",
    "#Load the training dataset and generate folds\n",
    "d = DataSet()\n",
    "folds,hold_out = kfold_split(d,n_folds=10)\n",
    "fold_stances, hold_out_stances = get_stances_for_folds(d,folds,hold_out)\n",
    "\n",
    "# Load the competition dataset\n",
    "competition_dataset = DataSet(\"competition_test\")\n",
    "X_competition, y_competition = generate_features(competition_dataset.stances, competition_dataset, \"competition\")\n",
    "\n",
    "Xs = dict()\n",
    "ys = dict()\n",
    "\n",
    "# Load/Precompute all features now\n",
    "X_holdout,y_holdout = generate_features(hold_out_stances,d,\"holdout\")\n",
    "for fold in fold_stances:\n",
    "    Xs[fold],ys[fold] = generate_features(fold_stances[fold],d,str(fold))\n",
    "\n",
    "\n",
    "best_score = 0\n",
    "best_fold = None\n",
    "\n",
    "\n",
    "# Classifier for each fold\n",
    "for fold in fold_stances:\n",
    "    ids = list(range(len(folds)))\n",
    "    del ids[fold]\n",
    "\n",
    "    X_train = np.vstack(tuple([Xs[i] for i in ids]))\n",
    "    y_train = np.hstack(tuple([ys[i] for i in ids]))\n",
    "\n",
    "    X_test = Xs[fold]\n",
    "    y_test = ys[fold]\n",
    "\n",
    "    clf = GradientBoostingClassifier(n_estimators=200, random_state=14128, verbose=True)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    predicted = [LABELS[int(a)] for a in clf.predict(X_test)]\n",
    "    actual = [LABELS[int(a)] for a in y_test]\n",
    "\n",
    "    fold_score, _ = score_submission(actual, predicted)\n",
    "    max_fold_score, _ = score_submission(actual, actual)\n",
    "\n",
    "    score = fold_score/max_fold_score\n",
    "\n",
    "    print(\"Score for fold \"+ str(fold) + \" was - \" + str(score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_fold = clf\n",
    "\n",
    "\n",
    "#Run on Holdout set and report the final score on the holdout set\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_holdout)]\n",
    "actual = [LABELS[int(a)] for a in y_holdout]\n",
    "\n",
    "print(\"Scores on the dev set\")\n",
    "report_score(actual,predicted)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "#Run on competition dataset\n",
    "predicted = [LABELS[int(a)] for a in best_fold.predict(X_competition)]\n",
    "actual = [LABELS[int(a)] for a in y_competition]\n",
    "\n",
    "print(\"Scores on the test set\")\n",
    "report_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
